{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1601648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Устройство (CPU или GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Гиперпараметры\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Функция для чтения ru.dic\n",
    "def read_ru_dic(file_path):\n",
    "    word_to_phonemes = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            match = re.match(r'^(.+?)\\s+(.+)$', line)\n",
    "            if match:\n",
    "                word_with_number = match.group(1)\n",
    "                transcription = match.group(2)\n",
    "                word_to_phonemes[word_with_number] = transcription.split()\n",
    "            else:\n",
    "                print(f\"Строка пропущена: {line}\")\n",
    "    return word_to_phonemes\n",
    "\n",
    "# Функция для чтения ru_emphasize.dict\n",
    "def read_ru_emphasize_dict(file_path):\n",
    "    word_to_stressed_word = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('|')\n",
    "            if len(parts) == 2:\n",
    "                word_with_number = parts[0]\n",
    "                stressed_word = parts[1]\n",
    "                word_to_stressed_word[word_with_number] = stressed_word\n",
    "            else:\n",
    "                print(f\"Строка пропущена: {line}\")\n",
    "    return word_to_stressed_word\n",
    "\n",
    "# Считываем данные из файлов\n",
    "word_to_phonemes = read_ru_dic('ru.dic')\n",
    "word_to_stressed_word = read_ru_emphasize_dict('ru_emphasize.dict')\n",
    "\n",
    "# Создаем датасет\n",
    "data = []\n",
    "for word_with_number, phoneme_seq in word_to_phonemes.items():\n",
    "    # Получаем слово с ударением (если есть)\n",
    "    input_word = word_to_stressed_word.get(word_with_number, word_with_number)\n",
    "    data.append((input_word, phoneme_seq))\n",
    "    \n",
    "# Учет буквы 'ё' как отдельного символа, всегда ударного\n",
    "for idx, (input_word, phoneme_seq) in enumerate(data):\n",
    "    # Если в слове есть 'ё', добавляем '+' перед ней\n",
    "    if 'ё' in input_word:\n",
    "        input_word = input_word.replace('ё', '+ё')\n",
    "        data[idx] = (input_word, phoneme_seq)\n",
    "        \n",
    "# Создание словаря символов (включая '+')\n",
    "char_counter = Counter()\n",
    "for input_word, _ in data:\n",
    "    char_counter.update(list(input_word))\n",
    "\n",
    "char_vocab = {char: idx+1 for idx, (char, _) in enumerate(char_counter.most_common())}\n",
    "char_vocab['<PAD>'] = 0\n",
    "char_vocab['<SOS>'] = len(char_vocab)\n",
    "char_vocab['<EOS>'] = len(char_vocab)\n",
    "char_vocab['<UNK>'] = len(char_vocab)\n",
    "\n",
    "idx2char = {idx: char for char, idx in char_vocab.items()}\n",
    "\n",
    "# Создание словаря фонем\n",
    "phoneme_counter = Counter()\n",
    "for _, phoneme_seq in data:\n",
    "    phoneme_counter.update(phoneme_seq)\n",
    "\n",
    "phoneme_vocab = {ph: idx+1 for idx, (ph, _) in enumerate(phoneme_counter.most_common())}\n",
    "phoneme_vocab['<PAD>'] = 0\n",
    "phoneme_vocab['<SOS>'] = len(phoneme_vocab)\n",
    "phoneme_vocab['<EOS>'] = len(phoneme_vocab)\n",
    "phoneme_vocab['<UNK>'] = len(phoneme_vocab)\n",
    "\n",
    "idx2phoneme = {idx: ph for ph, idx in phoneme_vocab.items()}\n",
    "\n",
    "# Функции для кодирования\n",
    "def encode_word(word):\n",
    "    return [char_vocab.get(c, char_vocab['<UNK>']) for c in word]\n",
    "\n",
    "def encode_phonemes(phoneme_seq):\n",
    "    return [phoneme_vocab.get(ph, phoneme_vocab['<UNK>']) for ph in phoneme_seq]\n",
    "\n",
    "# Кодируем данные\n",
    "encoded_words = [encode_word(input_word) for input_word, _ in data]\n",
    "encoded_phonemes = [encode_phonemes(phoneme_seq) for _, phoneme_seq in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "245fa7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "        self.input_lengths = [len(seq) for seq in inputs]\n",
    "        self.target_lengths = [len(seq) for seq in targets]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.inputs[idx], self.targets[idx],\n",
    "                self.input_lengths[idx], self.target_lengths[idx])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets, input_lengths, target_lengths = zip(*batch)\n",
    "\n",
    "    max_input_len = max(input_lengths)\n",
    "    max_target_len = max(target_lengths)\n",
    "\n",
    "    padded_inputs = [seq + [char_vocab['<PAD>']] * (max_input_len - len(seq)) for seq in inputs]\n",
    "    padded_targets = [[phoneme_vocab['<SOS>']] + seq + [phoneme_vocab['<EOS>']] + [phoneme_vocab['<PAD>']] * (max_target_len - len(seq)) for seq in targets]\n",
    "\n",
    "    return (torch.tensor(padded_inputs, dtype=torch.long),\n",
    "            torch.tensor(padded_targets, dtype=torch.long),\n",
    "            torch.tensor(input_lengths, dtype=torch.long),\n",
    "            torch.tensor([len(seq)+2 for seq in targets], dtype=torch.long))  # +2 для <SOS> и <EOS>\n",
    "\n",
    "dataset = G2PDataset(encoded_words, encoded_phonemes)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c899ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim, padding_idx=char_vocab['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input_seqs, input_lengths):\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        outputs, (hidden, cell) = self.lstm(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_dim, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim, padding_idx=phoneme_vocab['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seqs, hidden):\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acb408ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для сохранения чекпойнта\n",
    "def save_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, epoch, loss, filepath):\n",
    "    checkpoint = {\n",
    "        'encoder_state_dict': encoder.state_dict(),\n",
    "        'decoder_state_dict': decoder.state_dict(),\n",
    "        'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "        'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Модель сохранена в {filepath}\")\n",
    "\n",
    "# Функция для загрузки чекпойнта\n",
    "def load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, filepath):\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Модель загружена из {filepath}, Эпоха: {epoch}, Ошибка: {loss:.4f}\")\n",
    "    return epoch, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf49bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(char_vocab), EMBEDDING_DIM, HIDDEN_SIZE).to(device)\n",
    "decoder = Decoder(len(phoneme_vocab), EMBEDDING_DIM, HIDDEN_SIZE).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=phoneme_vocab['<PAD>'])\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60efd982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха [1/20], Ошибка: 0.0166\n",
      "Эпоха [2/20], Ошибка: 0.0166\n",
      "Эпоха [3/20], Ошибка: 0.0166\n",
      "Эпоха [4/20], Ошибка: 0.0164\n",
      "Эпоха [5/20], Ошибка: 0.0165\n",
      "Эпоха [6/20], Ошибка: 0.0165\n",
      "Эпоха [7/20], Ошибка: 0.0164\n",
      "Эпоха [8/20], Ошибка: 0.0165\n",
      "Эпоха [9/20], Ошибка: 0.0164\n",
      "Эпоха [10/20], Ошибка: 0.0162\n",
      "Эпоха [11/20], Ошибка: 0.0164\n",
      "Эпоха [12/20], Ошибка: 0.0160\n",
      "Эпоха [13/20], Ошибка: 0.0162\n",
      "Эпоха [14/20], Ошибка: 0.0161\n",
      "Эпоха [15/20], Ошибка: 0.0160\n",
      "Эпоха [16/20], Ошибка: 0.0161\n",
      "Эпоха [17/20], Ошибка: 0.0162\n",
      "Эпоха [18/20], Ошибка: 0.0161\n",
      "Эпоха [19/20], Ошибка: 0.0162\n",
      "Эпоха [20/20], Ошибка: 0.0160\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_seqs, target_seqs, input_lengths, target_lengths = batch\n",
    "        input_seqs = input_seqs.to(device)\n",
    "        target_seqs = target_seqs.to(device)\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_seqs, input_lengths)\n",
    "\n",
    "        # Подготовка входов для декодера\n",
    "        decoder_input = target_seqs[:, :-1]  # Убираем <EOS>\n",
    "        decoder_target = target_seqs[:, 1:]  # Убираем <SOS>\n",
    "\n",
    "        # Обучение с принудительным учителем (teacher forcing)\n",
    "        decoder_outputs, _ = decoder(decoder_input, encoder_hidden)\n",
    "\n",
    "        # Вычисление ошибки\n",
    "        loss = criterion(decoder_outputs.reshape(-1, decoder_outputs.size(-1)),\n",
    "                         decoder_target.reshape(-1))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Эпоха [{epoch+1}/{NUM_EPOCHS}], Ошибка: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1f500b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель сохранена в g2p_v2_e60\n"
     ]
    }
   ],
   "source": [
    "# После завершения эпохи обучения\n",
    "save_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, epoch+1, avg_loss, 'g2p_v2_e60')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "880eea6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель загружена из g2p_v2_e40, Эпоха: 20, Ошибка: 0.0166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20, 0.01655644970238751)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, 'g2p_v2_e40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "07feeeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово: хуй, Предсказанные фонемы: h u0 j\n",
      "Слово: пизда, Предсказанные фонемы: pj i0 z d a0\n",
      "Слово: п+езда, Предсказанные фонемы: pj e1 z d a0\n",
      "Слово: пизд+а, Предсказанные фонемы: pj i0 z d a0\n",
      "Слово: джигурда, Предсказанные фонемы: d zh i1 g u0 r d a0\n",
      "Слово: джигурд+а, Предсказанные фонемы: d zh i1 g u0 r d a0\n",
      "Слово: говножуй, Предсказанные фонемы: g o0 v n o0 zh u1 j\n",
      "Слово: говн+о, Предсказанные фонемы: g o1 v n o0\n",
      "Слово: уёбок, Предсказанные фонемы: u0 j o1 b o0 k\n",
      "Слово: уебище, Предсказанные фонемы: u0 j e1 bj i0 sch e0\n",
      "Слово: уебан, Предсказанные фонемы: u0 j e1 b a0 n\n",
      "Слово: уёбище, Предсказанные фонемы: u0 j o1 bj i0 sch e0\n"
     ]
    }
   ],
   "source": [
    "def predict(word):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_seq = torch.tensor([encode_word(word)], dtype=torch.long).to(device)\n",
    "        input_length = torch.tensor([len(input_seq[0])], dtype=torch.long).to(device)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_seq, input_length)\n",
    "\n",
    "        decoder_input = torch.tensor([[phoneme_vocab['<SOS>']]], dtype=torch.long).to(device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_phonemes = []\n",
    "\n",
    "        for _ in range(50):  # Максимальная длина последовательности фонем\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            phoneme_idx = topi.item()\n",
    "            if phoneme_idx == phoneme_vocab['<EOS>'] or phoneme_idx == phoneme_vocab['<PAD>']:\n",
    "                break\n",
    "            else:\n",
    "                decoded_phonemes.append(idx2phoneme[phoneme_idx])\n",
    "\n",
    "            decoder_input = torch.tensor([[phoneme_idx]], dtype=torch.long).to(device)\n",
    "\n",
    "        return ' '.join(decoded_phonemes)\n",
    "\n",
    "# Пример использования\n",
    "test_words = ['хуй', 'пизда', 'п+езда', 'пизд+а', 'джигурда', 'джигурд+а', 'говножуй', 'говн+о','уёбок', 'уебище','уебан', 'уёбище']\n",
    "for test_word in test_words:\n",
    "    predicted_phonemes = predict(test_word)\n",
    "    print(f'Слово: {test_word}, Предсказанные фонемы: {predicted_phonemes}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee478d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c5d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7194ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb6246a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942677bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698584c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11bbed26",
   "metadata": {},
   "source": [
    "## форматируем файл метадаты с применением этой g2p модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7df24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294e5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "37992b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_line(line, predict_function, special_space=\"<space>\"):\n",
    "    \"\"\"\n",
    "    Обрабатывает одну строку из metadata.csv и преобразует текст в фонемную транскрипцию,\n",
    "    сохраняя пунктуацию и заменяя пробелы между словами на специальный символ.\n",
    "\n",
    "    Args:\n",
    "        line (str): Строка из metadata.csv в формате 'filename|text'.\n",
    "        predict_function (function): Функция для преобразования слова в фонемы, например, predict(word).\n",
    "        special_space (str): Специальный символ для замены пробелов между словами.\n",
    "\n",
    "    Returns:\n",
    "        str: Новая строка в формате 'filename|phoneme1 phoneme2 <\\space> phoneme3 ...'.\n",
    "    \"\"\"\n",
    "    # Удаляем пробельные символы в начале и конце строки\n",
    "    line = line.strip()\n",
    "\n",
    "    # Проверяем наличие вертикальной черты '|'\n",
    "    if '|' not in line:\n",
    "        print(f\"Строка пропущена: нет символа '|' в строке: {line}\")\n",
    "        return None\n",
    "\n",
    "    # Разделяем строку на имя файла и текст\n",
    "    filename, text = line.split('|', 1)\n",
    "\n",
    "    # Регулярное выражение для разделения слов (включая '+') и пунктуации\n",
    "    # Это выражение захватывает слова, которые могут содержать '+', а также отдельные знаки пунктуации\n",
    "    tokens = re.findall(r'\\w+\\+\\w+|\\w+|[.,;:!?«»“”…()\\[\\]{}\\-]', text)\n",
    "\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.match(r'[.,;:!?«»“”…()\\[\\]{}\\-]', token):\n",
    "            # Если токен - пунктуация, добавляем его как есть\n",
    "            processed_tokens.append(token)\n",
    "        else:\n",
    "            # Иначе, токен - слово (может содержать '+')\n",
    "            word_lower = token.lower()\n",
    "            try:\n",
    "                # Преобразуем слово в фонемную транскрипцию\n",
    "                phoneme = predict_function(word_lower)\n",
    "                if phoneme:\n",
    "                    processed_tokens.append(phoneme)\n",
    "                else:\n",
    "                    print(f\"Пустая транскрипция для слова '{word_lower}'. Используем '<UNK>'.\")\n",
    "                    processed_tokens.append('<UNK>')  # Используем <UNK> для неизвестных фонем\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при предсказании слова '{word_lower}': {e}. Используем '<UNK>'.\")\n",
    "                processed_tokens.append('<UNK>')\n",
    "\n",
    "    # Теперь собираем новую строку с правильными разделителями\n",
    "    final_tokens = []\n",
    "    for i, token in enumerate(processed_tokens):\n",
    "        final_tokens.append(token)\n",
    "        if i < len(processed_tokens) - 1:\n",
    "            next_token = processed_tokens[i + 1]\n",
    "            # Определяем тип текущего и следующего токена\n",
    "            current_is_punct = re.match(r'[.,;:!?«»“”…()\\[\\]{}\\-]', token)\n",
    "            next_is_punct = re.match(r'[.,;:!?«»“”…()\\[\\]{}\\-]', next_token)\n",
    "\n",
    "            if not current_is_punct and not next_is_punct:\n",
    "                # Оба токена - слова, вставляем специальный символ с пробелами\n",
    "                final_tokens.append(f\" {special_space} \")\n",
    "            elif not current_is_punct and next_is_punct:\n",
    "                # Текущий токен - слово, следующий - пунктуация, вставляем пробел\n",
    "                final_tokens.append(' ')\n",
    "            elif current_is_punct and not next_is_punct:\n",
    "                # Текущий токен - пунктуация, следующий - слово, вставляем пробел\n",
    "                final_tokens.append(' ')\n",
    "            elif current_is_punct and next_is_punct:\n",
    "                # Оба токена - пунктуация, вставляем пробел\n",
    "                final_tokens.append(' ')\n",
    "            else:\n",
    "                # В остальных случаях, вставляем пробел\n",
    "                final_tokens.append(' ')\n",
    "\n",
    "    # Объединяем токены в строку\n",
    "    phoneme_text = ''.join(final_tokens)\n",
    "\n",
    "    # Заменяем множественные пробелы на один\n",
    "    phoneme_text = re.sub(r'\\s+', ' ', phoneme_text)\n",
    "\n",
    "    # Формируем новую строку\n",
    "    new_line = f\"{filename}|{phoneme_text}\\n\"\n",
    "\n",
    "    return new_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a64123eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходная строка:\n",
      "000012_RUSLAN|Ведь представл+ения, которые он+о рождает, безграничны д+о нул+я.\n",
      "\n",
      "Обработанная строка:\n",
      "000012_RUSLAN|vj e1 dj <space> p rj e0 d s t a0 v lj e1 nj i0 j a0 , k o0 t o1 r y0 j e0 <space> o0 n o1 <space> r o0 zh d a1 j e0 t , bj e0 z g r a0 nj i1 ch n y0 <space> d o1 <space> n u0 lj a1 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Пример строки из metadata.csv\n",
    "test_line = \"000012_RUSLAN|Ведь представл+ения, которые он+о рождает, безграничны д+о нул+я.\"\n",
    "\n",
    "# Обработка строки\n",
    "processed_line = process_line(test_line, predict)\n",
    "\n",
    "if processed_line:\n",
    "    print(\"Исходная строка:\")\n",
    "    print(test_line)\n",
    "    print(\"\\nОбработанная строка:\")\n",
    "    print(processed_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c28dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8ba260e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_metadata_to_phonemes(input_metadata_path, output_metadata_phonemes_path, predict_function, special_space=\"<\\\\space>\"):\n",
    "    \"\"\"\n",
    "    Преобразует metadata.csv в metadata_phonemes.csv с фонемными транскрипциями.\n",
    "    \n",
    "    Args:\n",
    "        input_metadata_path (str): Путь к оригинальному metadata.csv\n",
    "        output_metadata_phonemes_path (str): Путь для сохранения фонемного metadata\n",
    "        predict_function (function): Функция для преобразования слова в фонемы, например, predict(word)\n",
    "        special_space (str): Специальный символ для замены пробелов между словами.\n",
    "    \"\"\"\n",
    "    with open(input_metadata_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_metadata_phonemes_path, 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        for line_num, line in enumerate(infile, 1):\n",
    "            processed_line = process_line(line, predict_function, special_space)\n",
    "            if processed_line:\n",
    "                outfile.write(processed_line)\n",
    "            else:\n",
    "                print(f\"Строка {line_num} пропущена.\")\n",
    "    \n",
    "    print(f\"Фонемный файл сохранен по пути: {output_metadata_phonemes_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8ce5411f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фонемный файл сохранен по пути: /app/TTSproject/tacotron/datasets/metadata_phonemes.csv\n"
     ]
    }
   ],
   "source": [
    "# Пути к файлам\n",
    "input_metadata_path = '/app/TTSproject/tacotron/datasets/metadata.csv'\n",
    "output_metadata_phonemes_path = '/app/TTSproject/tacotron/datasets/metadata_phonemes.csv'\n",
    "\n",
    "# Вызов функции преобразования\n",
    "convert_metadata_to_phonemes(input_metadata_path, output_metadata_phonemes_path, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718f933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "82efed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_punctuation(file_path):\n",
    "    \"\"\"\n",
    "    Заменяет специфические символы в файле metadata.csv.\n",
    "    \n",
    "    Замены:\n",
    "        « » “ ” -> \"\n",
    "        … -> ...\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Путь к файлу metadata.csv\n",
    "    \"\"\"\n",
    "    replacements = {\n",
    "        '«': '\"',\n",
    "        '»': '\"',\n",
    "        '“': '\"',\n",
    "        '”': '\"',\n",
    "        #'…': '...',\n",
    "    }\n",
    "    \n",
    "    # Чтение содержимого файла\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Выполнение замен\n",
    "    for old, new in replacements.items():\n",
    "        content = content.replace(old, new)\n",
    "    \n",
    "    # Запись изменённого содержимого обратно в файл\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Пример использования:\n",
    "replace_punctuation('/app/TTSproject/tacotron/datasets/metadata_phonemes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b43aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e5ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65504c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7070c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbae0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, predict_function, special_space=\"<space>\"):\n",
    "    # Регулярное выражение для разделения слов (включая '+') и пунктуации\n",
    "    # Это выражение захватывает слова, которые могут содержать '+', а также отдельные знаки пунктуации\n",
    "    tokens = re.findall(r'\\w+\\+\\w+|\\w+|[.,;:!?«»“”…()\\[\\]{}\\-]', text)\n",
    "\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.match(r'[.,;:!?«»“”…()\\[\\]{}\\-]', token):\n",
    "            # Если токен - пунктуация, добавляем его как есть\n",
    "            processed_tokens.append(token)\n",
    "        else:\n",
    "            # Иначе, токен - слово (может содержать '+')\n",
    "            word_lower = token.lower()\n",
    "            try:\n",
    "                # Преобразуем слово в фонемную транскрипцию\n",
    "                phoneme = predict_function(word_lower)\n",
    "                if phoneme:\n",
    "                    processed_tokens.append(phoneme)\n",
    "                else:\n",
    "                    print(f\"Пустая транскрипция для слова '{word_lower}'. Используем '<UNK>'.\")\n",
    "                    processed_tokens.append('<UNK>')  # Используем <UNK> для неизвестных фонем\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при предсказании слова '{word_lower}': {e}. Используем '<UNK>'.\")\n",
    "                processed_tokens.append('<UNK>')\n",
    "\n",
    "    # Теперь собираем новую строку с правильными разделителями\n",
    "    final_tokens = []\n",
    "    for i, token in enumerate(processed_tokens):\n",
    "        final_tokens.append(token)\n",
    "        if i < len(processed_tokens) - 1:\n",
    "            next_token = processed_tokens[i + 1]\n",
    "            # Определяем тип текущего и следующего токена\n",
    "            current_is_punct = re.match(r'[.,;:!?«»“”…()\\[\\]{}\\-]', token)\n",
    "            next_is_punct = re.match(r'[.,;:!?«»“”…()\\[\\]{}\\-]', next_token)\n",
    "\n",
    "            if not current_is_punct and not next_is_punct:\n",
    "                # Оба токена - слова, вставляем специальный символ с пробелами\n",
    "                final_tokens.append(f\" {special_space} \")\n",
    "            elif not current_is_punct and next_is_punct:\n",
    "                # Текущий токен - слово, следующий - пунктуация, вставляем пробел\n",
    "                final_tokens.append(' ')\n",
    "            elif current_is_punct and not next_is_punct:\n",
    "                # Текущий токен - пунктуация, следующий - слово, вставляем пробел\n",
    "                final_tokens.append(' ')\n",
    "            elif current_is_punct and next_is_punct:\n",
    "                # Оба токена - пунктуация, вставляем пробел\n",
    "                final_tokens.append(' ')\n",
    "            else:\n",
    "                # В остальных случаях, вставляем пробел\n",
    "                final_tokens.append(' ')\n",
    "\n",
    "    # Объединяем токены в строку\n",
    "    phoneme_text = ''.join(final_tokens)\n",
    "\n",
    "    # Заменяем множественные пробелы на один\n",
    "    phoneme_text = re.sub(r'\\s+', ' ', phoneme_text)\n",
    "\n",
    "    # Формируем новую строку\n",
    "    new_line = f\"{phoneme_text}\\n\"\n",
    "\n",
    "    return new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "019ca49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходная строка:\n",
      "Удивительно, почему +именно этот чекп+ойнт ещё умеет ставить плюсики.\n",
      "\n",
      "Обработанная строка:\n",
      "u0 dj i0 vj i1 tj e0 lj n o0 , p o0 ch e0 m u1 <space> i0 mj e0 n n o1 <space> e1 t o0 t <space> ch e0 k p o1 j n t <space> j e0 sch o1 <space> u0 mj e1 j e0 t <space> s t a1 vj i0 tj <space> p lj u1 sj i0 kj i0 .\n"
     ]
    }
   ],
   "source": [
    "# Пример строки из metadata.csv\n",
    "test_text = \"Удивительно, почему +именно этот чекп+ойнт ещё умеет ставить плюсики.\"\n",
    "\n",
    "# Обработка строки\n",
    "processed_text = process_text(test_text, predict)\n",
    "\n",
    "if processed_txt:\n",
    "    print(\"Исходная строка:\")\n",
    "    print(test_text)\n",
    "    print(\"\\nОбработанная строка:\")\n",
    "    print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5fa4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_text(text, predict_function, special_space=\"<space>\"):\n",
    "    # Регулярное выражение для разделения слов (включая '+') и пунктуации\n",
    "    # Это выражение захватывает слова, которые могут содержать '+', а также отдельные знаки пунктуации\n",
    "    tokens = re.findall(r'\\w+\\+\\w+|\\w+|[.,;:!?«»“”…()\\[\\]{}\\-]', text)\n",
    "\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.match(r'[.,;:!?«»“”…()\\[\\]{}\\-]', token):\n",
    "            # Если токен - пунктуация, добавляем его как есть\n",
    "            processed_tokens.append(token)\n",
    "        else:\n",
    "            # Иначе, токен - слово (может содержать '+')\n",
    "            word_lower = token.lower()\n",
    "            try:\n",
    "                # Преобразуем слово в фонемную транскрипцию\n",
    "                phoneme = predict_function(word_lower)\n",
    "                if phoneme:\n",
    "                    processed_tokens.append(phoneme)\n",
    "                else:\n",
    "                    print(f\"Пустая транскрипция для слова '{word_lower}'. Используем '<UNK>'.\")\n",
    "                    processed_tokens.append('<UNK>')  # Используем <UNK> для неизвестных фонем\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при предсказании слова '{word_lower}': {e}. Используем '<UNK>'.\")\n",
    "                processed_tokens.append('<UNK>')\n",
    "\n",
    "    # Теперь собираем новую строку с правильными разделителями\n",
    "    final_tokens = []\n",
    "    for i, token in enumerate(processed_tokens):\n",
    "        final_tokens.append(token)\n",
    "        if i < len(processed_tokens) - 1:\n",
    "            next_token = processed_tokens[i + 1]\n",
    "            # Определяем тип текущего и следующего токена\n",
    "            current_is_punct = re.match(r'[.,;:!?«»“”…()\\[\\]{}\\-]', token)\n",
    "            next_is_punct = re.match(r'[.,;:!?«»“”…()\\[\\]{}\\-]', next_token)\n",
    "\n",
    "            if not current_is_punct and not next_is_punct:\n",
    "                # Оба токена - слова, вставляем специальный символ с пробелами\n",
    "                final_tokens.append(f\" {special_space} \")\n",
    "            elif not current_is_punct and next_is_punct:\n",
    "                # Текущий токен - слово, следующий - пунктуация, вставляем пробел\n",
    "                final_tokens.append(' ')\n",
    "            elif current_is_punct and not next_is_punct:\n",
    "                # Текущий токен - пунктуация, следующий - слово, вставляем пробел\n",
    "                final_tokens.append(' ')\n",
    "            elif current_is_punct and next_is_punct:\n",
    "                # Оба токена - пунктуация, вставляем пробел\n",
    "                final_tokens.append(' ')\n",
    "            else:\n",
    "                # В остальных случаях, вставляем пробел\n",
    "                final_tokens.append(' ')\n",
    "\n",
    "    # Объединяем токены в строку\n",
    "    phoneme_text = ''.join(final_tokens)\n",
    "\n",
    "    # Заменяем множественные пробелы на один\n",
    "    phoneme_text = re.sub(r'\\s+', ' ', phoneme_text)\n",
    "\n",
    "    # Формируем новую строку\n",
    "    new_line = f\"{phoneme_text}\"\n",
    "\n",
    "    return new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdcf05ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a1 h u0 j e0 tj , e1 t a0 <space> f u1 n k c i0 j a0 <space> rj e0 a1 lj n o0 <space> r a0 b o1 t a0 j e0 t <space> i0 z n u0 t rj i1 <space> tj e1 k u0 sch e0 g o0 <space> f a1 j l a0 !'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text('ахуеть, эта функция реально работает изнутр+и т+екущего файла!', predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c86142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f2512f7",
   "metadata": {},
   "source": [
    "## тесты обновления датасета для такотрона на фонемный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "27151fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "# #### changed version for ru text\n",
    "\n",
    "_pad = '_'\n",
    "_punctuation = ' -,.!?\\'\\\"():;'\n",
    "_special = '+'\n",
    "_letters = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'  # АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ\n",
    "\n",
    "# Все предопределенные символы в одном списке для сортировки\n",
    "predefined_order = [_pad] + list(_punctuation) + list(_special)  # + list(_letters)\n",
    "\n",
    "def get_unique_symbols_from_metadata(file_path, predefined_order):\n",
    "    \"\"\"\n",
    "    Считывает файл metadata.csv и возвращает список уникальных токенов (фонем, знаков препинания, специальных символов).\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Путь к файлу metadata.csv\n",
    "        predefined_order (list): Список предопределённых символов\n",
    "    \n",
    "    Returns:\n",
    "        list: Список уникальных символов, включающий предопределённые и дополнительные\n",
    "    \"\"\"\n",
    "    unique_symbols = set()\n",
    "    \n",
    "    # Увеличиваем максимальный размер поля для CSV\n",
    "    maxInt = sys.maxsize\n",
    "    while True:\n",
    "        try:\n",
    "            csv.field_size_limit(maxInt)\n",
    "            break\n",
    "        except OverflowError:\n",
    "            maxInt = int(maxInt / 10)\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter='|')\n",
    "        for row in reader:\n",
    "            if len(row) > 1:\n",
    "                phoneme_text = row[1].strip()\n",
    "                tokens = phoneme_text.split()  # Разделение по пробелам\n",
    "                for token in tokens:\n",
    "                    # Очистка токена от лишних символов\n",
    "                    token_clean = token.strip().replace('\\n', '').replace('\\r', '')\n",
    "                    if token_clean:\n",
    "                        unique_symbols.add(token_clean)\n",
    "            else:\n",
    "                print(f\"Строка пропущена: {row}\")\n",
    "    \n",
    "    # Добавляем предопределённые символы\n",
    "    sorted_symbols = predefined_order.copy()\n",
    "    \n",
    "    # Добавляем новые уникальные символы, отсортированные\n",
    "    extra_symbols = sorted([s for s in unique_symbols if s not in predefined_order])\n",
    "    \n",
    "    return sorted_symbols + extra_symbols\n",
    "\n",
    "# Пример использования:\n",
    "file_path = '/app/TTSproject/tacotron/datasets/metadata.csv'\n",
    "unique_symbols = get_unique_symbols_from_metadata(file_path, predefined_order)\n",
    "symbols = unique_symbols\n",
    "\n",
    "# Создание словарей\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e51ff961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b4f95fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ru_text(text):\n",
    "    # Конвертируем текст в список ID\n",
    "    #sequence = [_symbol_to_id[s] for s in text if s in _symbol_to_id]\n",
    "    sequence = [_symbol_to_id[s.lower()] for s in text if s.lower() in _symbol_to_id]\n",
    "    \n",
    "    # Преобразуем список ID в тензор\n",
    "    sequence_tensor = torch.tensor(sequence, dtype=torch.int64)\n",
    "    \n",
    "    return sequence_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "53c108e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([46, 15, 60, 56, 37, 23, 20, 24, 41, 31, 33, 23])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ru_phonemes('r a0 z v lj e0 ch e1 nj i0 j e0 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d6dbb992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r a0 z v lj e0 ch e1 nj i0 j e0'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_ru_phonemes(get_ru_phonemes('r a0 z v lj e0 ch e1 nj i0 j e0 '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ce2e4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ru_phonemes(text):\n",
    "    \"\"\"\n",
    "    Конвертирует текст с фонемами в тензор ID.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Строка с фонемами, разделёнными пробелами.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Тензор с ID фонем.\n",
    "    \"\"\"\n",
    "    # Разбиваем текст на токены (фонемы, знаки препинания, специальные символы)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Преобразуем каждый токен в соответствующий ID, используя словарь _symbol_to_id\n",
    "    sequence = [_symbol_to_id[token.lower()] for token in tokens if token.lower() in _symbol_to_id]\n",
    "    \n",
    "    # Преобразуем список ID в тензор\n",
    "    sequence_tensor = torch.tensor(sequence, dtype=torch.int64)\n",
    "    \n",
    "    return sequence_tensor\n",
    "\n",
    "def tensor_to_ru_phonemes(tensor):\n",
    "    \"\"\"\n",
    "    Конвертирует тензор ID фонем обратно в текст.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): Тензор с ID фонем.\n",
    "    \n",
    "    Returns:\n",
    "        str: Строка с фонемами, разделёнными пробелами.\n",
    "    \"\"\"\n",
    "    # Преобразуем тензор в список ID\n",
    "    ids = tensor.tolist()\n",
    "    \n",
    "    # Преобразуем каждый ID обратно в соответствующий токен, используя словарь _id_to_symbol\n",
    "    tokens = [ _id_to_symbol.get(id, '<UNK>') for id in ids ]\n",
    "    \n",
    "    # Объединяем токены в строку с пробелами между ними\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03151dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7ab54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246e238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411f52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca53a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4779ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b662160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae3df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc6661bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель загружена из g2p_v2_e40, Эпоха: 20, Ошибка: 0.0166\n",
      "Фонемная транскрипция для 'ч+увство': <\\space> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <\\space> me.nia1 <UNK> <UNK> <\\space> <UNK> <UNK> <UNK> <\\space> <UNK> <UNK> <UNK> <UNK> <UNK> <\\space> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> me.nia1 <UNK> <UNK> <UNK> <\\space> <UNK> <UNK> <UNK> <UNK> <UNK> me.nia1 <\\space> <UNK> <UNK> <SOS> <UNK> <UNK> <\\space> <UNK> <UNK> <UNK>\n"
     ]
    }
   ],
   "source": [
    "# g2p_infer.py\n",
    "\n",
    "import torch\n",
    "import re\n",
    "from g2p import * # Убедитесь, что пути импорта корректны\n",
    "from collections import Counter\n",
    "\n",
    "# Устройство (CPU или GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Загрузка словарей\n",
    "_symbol_to_id = {\n",
    "    # Пример, замените на ваш фактический словарь\n",
    "    'su.et.noe': 1,\n",
    "    '<\\\\space>': 2,\n",
    "    'chu.vstvo1': 3,\n",
    "    'tre.vo.zhit': 4,\n",
    "    'me.nia1': 5,\n",
    "    '.': 6,\n",
    "    '<UNK>': 7,\n",
    "    '<SOS>': 8,\n",
    "    '<EOS>': 9,\n",
    "    '_': 0\n",
    "}\n",
    "\n",
    "_id_to_symbol = {v: k for k, v in _symbol_to_id.items()}\n",
    "\n",
    "# Функции для кодирования и декодирования\n",
    "def encode_phonemes(phoneme_seq):\n",
    "    return [ _symbol_to_id.get(ph, _symbol_to_id['<UNK>']) for ph in phoneme_seq.split() ]\n",
    "\n",
    "def decode_phonemes(tensor):\n",
    "    return ' '.join([ _id_to_symbol.get(id, '<UNK>') for id in tensor.tolist() ])\n",
    "\n",
    "# Загрузка модели\n",
    "def load_g2p_model(encoder_path, decoder_path):\n",
    "    # Инициализация моделей\n",
    "    encoder = Encoder(input_size=len(_symbol_to_id), embedding_dim=128, hidden_size=256).to(device)\n",
    "    decoder = Decoder(output_size=len(_symbol_to_id), embedding_dim=128, hidden_size=256).to(device)\n",
    "    \n",
    "    # Загрузка весов\n",
    "    encoder.load_state_dict(torch.load(encoder_path, map_location=device))\n",
    "    decoder.load_state_dict(torch.load(decoder_path, map_location=device))\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    return encoder, decoder\n",
    "\n",
    "# Функция инференса\n",
    "def predict_word(word, encoder, decoder):\n",
    "    \"\"\"\n",
    "    Преобразует слово в фонемную транскрипцию с помощью модели G2P.\n",
    "    \n",
    "    Args:\n",
    "        word (str): Входное слово.\n",
    "        encoder (nn.Module): Загруженный энкодер модели.\n",
    "        decoder (nn.Module): Загруженный декодер модели.\n",
    "    \n",
    "    Returns:\n",
    "        str: Предсказанная фонемная транскрипция.\n",
    "    \"\"\"\n",
    "    # Очистка и подготовка слова\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Кодирование слова\n",
    "    input_seq = encode_phonemes(word)\n",
    "    input_tensor = torch.tensor([input_seq], dtype=torch.long).to(device)\n",
    "    input_length = torch.tensor([len(input_seq)], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor, input_length)\n",
    "        decoder_input = torch.tensor([[ _symbol_to_id['<SOS>'] ]], dtype=torch.long).to(device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_phonemes = []\n",
    "        \n",
    "        for _ in range(50):  # Максимальная длина последовательности фонем\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            phoneme_idx = topi.item()\n",
    "            phoneme = _id_to_symbol.get(phoneme_idx, '<UNK>')\n",
    "            decoded_phonemes.append(phoneme)\n",
    "                \n",
    "            decoder_input = torch.tensor([[phoneme_idx]], dtype=torch.long).to(device)\n",
    "    \n",
    "    return ' '.join(decoded_phonemes)\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, 'g2p_v2_e40')\n",
    "    \n",
    "    # Пример слова\n",
    "    test_word = \"ч+увство\"\n",
    "    \n",
    "    # Инференс\n",
    "    predicted_phonemes = predict_word(test_word, encoder, decoder)\n",
    "    print(f\"Фонемная транскрипция для '{test_word}': {predicted_phonemes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dcff0a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_phonemes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredicted_phonemes\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_phonemes' is not defined"
     ]
    }
   ],
   "source": [
    "predicted_phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a2905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
